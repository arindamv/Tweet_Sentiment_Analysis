{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Z7Udev3FEz6",
        "-mNYMR3EFHNo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-kuFJ7GCNU"
      },
      "source": [
        "#Gaurav + Manisha + Qayem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGHRHVuZtHIH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiHgNGC6GriZ",
        "outputId": "6649ff36-d769-4d01-9377-46ba4f5f9348"
      },
      "source": [
        "!pip install chart_studio\n",
        "!pip install scattertext\n",
        "!python -m spacy download en_core_web_md "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chart_studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart_studio) (4.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart_studio) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2.10)\n",
            "Installing collected packages: chart-studio\n",
            "Successfully installed chart-studio-1.1.0\n",
            "Collecting scattertext\n",
            "  Downloading scattertext-0.1.5-py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.19.5)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.10.2)\n",
            "Collecting gensim>=4.0.0\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.4.1)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.0.1)\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.1.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->scattertext) (5.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (1.1.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scattertext) (0.5.2)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9310 sha256=50d79a1ab35775b5d980f82c4e03a89df39dad04bd7b0297fe4d5361c558ec48\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: mock, gensim, flashtext, scattertext\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed flashtext-2.7 gensim-4.1.2 mock-4.0.3 scattertext-0.1.5\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051302 sha256=da89ac10f2b52add4f849f6a5746349b71232e66176a198128ceae61edf316f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-by3kipbb/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "s5_CYFX7L-bs",
        "outputId": "245d6275-d4b3-4915-dc23-e24a3b076ddd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import iplot\n",
        "import cufflinks\n",
        "pd.options.display.max_columns = 30\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import plotly.figure_factory as ff\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "cufflinks.go_offline()\n",
        "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "from collections import Counter\n",
        "import scattertext as st\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "import chart_studio\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-795435e920e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchart_studio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvSWp6oYuXnP"
      },
      "source": [
        "df1 = pd.read_csv('data1.csv')\n",
        "df2 = pd.read_csv('data2.csv')\n",
        "df3 = pd.read_csv('data3.csv')\n",
        "df4 = pd.read_csv('data4.csv')\n",
        "df5 = pd.read_csv('data5.csv')\n",
        "df6 = pd.read_csv('data6.csv')\n",
        "df7 = pd.read_csv('data7.csv')\n",
        "df8 = pd.read_csv('data8.csv')\n",
        "df9 = pd.read_csv('data9.csv')\n",
        "df10 = pd.read_csv('data10.csv')\n",
        "df11 = pd.read_csv('data11.csv')\n",
        "df12 = pd.read_csv('data12.csv')\n",
        "df13 = pd.read_csv('data13.csv')\n",
        "df14 = pd.read_csv('data14.csv')\n",
        "df15 = pd.read_csv('data15.csv')\n",
        "df16 = pd.read_csv('data16.csv')\n",
        "df17 = pd.read_csv('data17.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS716UdKug_x"
      },
      "source": [
        "df = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J3ip5--uncb"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U44_7bUoy_Ur"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEE2oJdJyYFe"
      },
      "source": [
        "#1. Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJTA6inMyhpz"
      },
      "source": [
        "##1.1. Adding Column Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H17_XvjKuudI"
      },
      "source": [
        "df.columns = ['Class', 'Id', 'Date', 'Flag', 'User', 'Tweet'];\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl4xZXJ8yns0"
      },
      "source": [
        "##1.2. Checking for Nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osXS8K5ovTlM"
      },
      "source": [
        "np.sum(df.isnull().any(axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXxYx2T3yuBM"
      },
      "source": [
        "##1.3. Conversion of Date to Python Datetime Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xzOzqVwvqKE"
      },
      "source": [
        "df.Date = pd.to_datetime(df.Date, format = '%a %b %d %X PDT %Y')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTk8yTy_z3Bq"
      },
      "source": [
        "##1.4. Dropping Surplus Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjG7_XZlwRHg"
      },
      "source": [
        "df['Flag']. value_counts() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWIAV17Tw0tg"
      },
      "source": [
        "df.drop('Flag', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF3fG31axPvf"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V89u8XbY0D5c"
      },
      "source": [
        "##1.5 Removing Duplicate Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPpITtK1xSVv"
      },
      "source": [
        "df.Id.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCKvKQ_0xXSZ"
      },
      "source": [
        "df[df.Id == 1753678185]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kp3CbCvx3U6"
      },
      "source": [
        "df = df.drop_duplicates(subset = 'Id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgsnbC7lyCgd"
      },
      "source": [
        "df.Id.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHgDkHopyGGm"
      },
      "source": [
        "df.Class.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoUb_0bU00gP"
      },
      "source": [
        "sns.countplot(x='Class', data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-OEx1Nc7Z_j"
      },
      "source": [
        "##1.6. Renaming the Classes to 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92diI7lV7Dp6"
      },
      "source": [
        "df['Class'] = df['Class'].replace(4,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYpf_Cln7w6H"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzBhzRIE772c"
      },
      "source": [
        "##1.7. Converting Tweets to Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FsHiv2B8DtQ"
      },
      "source": [
        "df.Tweet = df.Tweet.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXxFaCpl8QiI"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sJvvorQ8o5C"
      },
      "source": [
        "##1.8. Removing Stop Words and Letters Repeating More than 2 Times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9wQ2y42GLv7"
      },
      "source": [
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'([a-z])\\1+', r'\\1\\1', text)\n",
        "df['Tweet'] = df['Tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axXYazM-9IoC"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\", \"i'm\":\"i am\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
        "\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.remove('not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAWg2gLu8YJ1"
      },
      "source": [
        "df['TweetMinusStopWords'] = [neg_pattern.sub(lambda x: negations_dic[x.group()], x) for x in df.Tweet]\n",
        "\n",
        "df['TweetMinusStopWords'] = df['TweetMinusStopWords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIEznN8H9W-E"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dta1s0MC9yqM"
      },
      "source": [
        "##1.9. Removing @ and Hashtag Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGka9uw9uft"
      },
      "source": [
        "df['TweetMinusStopWords'] = df['TweetMinusStopWords'].apply(lambda x: ' '.join([word for word in x.split() if (re.match('^(?!@+).', word))]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYFWiCrB_fE"
      },
      "source": [
        "df['TweetMinusStopWords'] = df['TweetMinusStopWords'].apply(lambda x: ' '.join([word for word in x.split() if (re.match('^(?!#+).', word))]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcTjgvhjALLv"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRyVEOvqC5Cc"
      },
      "source": [
        "##1.10 Removing URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wt1uLAEBH42"
      },
      "source": [
        "df['TweetMinusStopWords'] = df['TweetMinusStopWords'].apply(lambda x: ' '.join([word for word in x.split() if (re.match('^(?!^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$).', word))]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMcfV2_wLnm"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofiqNHakxeQz"
      },
      "source": [
        "##1.11 Removing Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr3zT6lQwfZL"
      },
      "source": [
        "df['TweetMinusStopWords'] = df['TweetMinusStopWords'].apply(lambda x: re.sub('[0-9]+', '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEtok_DYx2RJ"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VToAVaFlyQs1"
      },
      "source": [
        "##1.12 Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzOtCUlIx7G0"
      },
      "source": [
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "df['TweetMinusStopWords'] = df['TweetMinusStopWords'].apply(lambda x: cleaning_punctuations(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tabrWPzDygb-"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjw1QveV1GX1"
      },
      "source": [
        "#1.13 Splitting String into List of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukc-m0HuzL_b"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'+')\n",
        "df['TweetTokens'] = df['TweetMinusStopWords'].apply(lambda x:str.split(x))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xV2ZK7o2vCE"
      },
      "source": [
        "##1.14 Getting Root Words (Stemming and Lemmatizing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB9LtF0V5dEs"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIo6pn811Z14"
      },
      "source": [
        "st = nltk.PorterStemmer()\n",
        "lm = nltk.WordNetLemmatizer()\n",
        "def lem(data):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return text\n",
        "\n",
        "df['TweetTokens'] = df['TweetTokens'].apply(lambda x: lem(x))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXSWBix63Yj5"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkgMgVC45L6R"
      },
      "source": [
        "def lem(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return text\n",
        "\n",
        "df['TweetTokensStemmed'] = df['TweetTokens'].apply(lambda x: lem(x))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksCqulybJb5O"
      },
      "source": [
        "#2. Preliminary Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR3oJJK7JhTC"
      },
      "source": [
        "##2.1. Most Common Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-5ZM4Bz7mnB"
      },
      "source": [
        "data_neg = df['TweetMinusStopWords']\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.title('Most Common Words Overall')\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800, collocations=False).generate(\" \".join(data_neg))\n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FoTZWJjBSzd"
      },
      "source": [
        "data_neg = df['TweetMinusStopWords'][df['Class'] == 0]\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.title('Most Common Words in Class 0')\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800, collocations=False).generate(\" \".join(data_neg))\n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4PLUX3-J3wP"
      },
      "source": [
        "data_neg = df['TweetMinusStopWords'][df['Class'] == 1]\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.title('Most Common Words in Class 4')\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800, collocations=False).generate(\" \".join(data_neg))\n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Be7CgDvRuy"
      },
      "source": [
        "##2.2. Most Common Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25y-_UCLD5c"
      },
      "source": [
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az7InuF2LOq6"
      },
      "source": [
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_bigram(df['TweetMinusStopWords'], 20)\n",
        "df3 = pd.DataFrame(common_words, columns = ['Tweet' , 'Count'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiO8M6lEJ8ww"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "df3.groupby('Tweet').sum()['Count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 Bigrams Overall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD2MeV0VK5Zk"
      },
      "source": [
        "common_words = get_top_n_bigram(df['TweetMinusStopWords'][df.Class == 0], 20)\n",
        "df1 = pd.DataFrame(common_words, columns = ['Tweet' , 'Count'])\n",
        "common_words = get_top_n_bigram(df['TweetMinusStopWords'][df.Class == 1], 20)\n",
        "df2 = pd.DataFrame(common_words, columns = ['Tweet' , 'Count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6ch5-f2otUh"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "df1.groupby('Tweet').sum()['Count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 Bigrams for Class 0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAJNjZvFoztO"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "df2.groupby('Tweet').sum()['Count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 Bigrams for Class 4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2dKsS_4vNnK"
      },
      "source": [
        "##2.3. Most Common Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Rxtwvoo31J"
      },
      "source": [
        "def get_top_n_trigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_trigram(df['TweetMinusStopWords'], 20)\n",
        "df4 = pd.DataFrame(common_words, columns = ['Tweet' , 'Count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgHkuiDZpVlU"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "df4.groupby('Tweet').sum()['Count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 Trigrams Overall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoiXgLzmpkCC"
      },
      "source": [
        "common_words = get_top_n_trigram(df['TweetMinusStopWords'][df.Class == 0], 20)\n",
        "df5 = pd.DataFrame(common_words, columns = ['Tweet' , 'Count'])\n",
        "common_words = get_top_n_trigram(df['TweetMinusStopWords'][df.Class == 1], 20)\n",
        "df6 = pd.DataFrame(common_words, columns = ['Tweet' , 'Count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_9onTicpvCI"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "df5.groupby('Tweet').sum()['Count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 Trigrams for Class 0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xYPvwQfp1M0"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "df6.groupby('Tweet').sum()['Count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 Trigrams for Class 4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMtDHfAevcIR"
      },
      "source": [
        "##2.3. Temporal Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfsdjFe9p7fr"
      },
      "source": [
        "bins = np.linspace(0, 24, 25)\n",
        "df['TimeBin'] = pd.cut(df.Date.dt.hour, bins, labels=np.linspace(1, 24, 24), right=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6KLkgfpv_Xj"
      },
      "source": [
        "df3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYFZOoSIeh8X"
      },
      "source": [
        "###2.3.1. Temporal Variation in Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEVvhWtM7fVZ"
      },
      "source": [
        "hourly = pd.DataFrame(data={'Hour': np.linspace(0, 24, 25)})\n",
        "for i in df3.Tweet:\n",
        "  h = df['TweetMinusStopWords'].apply(lambda x: x.count(i))  \n",
        "  d = pd.DataFrame(data={i: h})\n",
        "  d['TimeBin'] = df.TimeBin\n",
        "  g = d.groupby(\"TimeBin\")\n",
        "  g = g[i].sum()\n",
        "  hourly = pd.concat([hourly, g], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TSbc-9j-Qiw"
      },
      "source": [
        "hourly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf7zTMpDb04I"
      },
      "source": [
        "hourly.drop('Hour', axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEpJc49PBAZ9"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "hourly.iplot(yTitle='Count', xTitle = 'Hour of Day', title='Hourly Variation of Top Bigrams')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twQ-fK2OGLTl"
      },
      "source": [
        "daily = pd.DataFrame(data={'Day': np.linspace(0, 7, 8)})\n",
        "for i in df3.Tweet:\n",
        "  h = df['TweetMinusStopWords'].apply(lambda x: x.count(i))  \n",
        "  d = pd.DataFrame(data={i: h})\n",
        "  d['TimeBin'] = pd.DatetimeIndex(df['Date']).weekday\n",
        "  g = d.groupby(\"TimeBin\")\n",
        "  g = g[i].sum()\n",
        "  daily = pd.concat([daily, g], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozuC6K7lIqVf"
      },
      "source": [
        "daily"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9kHW7qgkLew"
      },
      "source": [
        "daily.drop('Day', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2lsgmQaJggk"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "daily.iplot(yTitle='Count', xTitle = 'Day of Week', title='Daily Variation of Top Bigrams')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gLzoc5OgKgX"
      },
      "source": [
        "###2.3.2 Temporal Variation in Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPIbyWzqJvF6"
      },
      "source": [
        "hourly = pd.DataFrame(data={'Hour': np.linspace(0, 24, 25)})\n",
        "for i in df4.Tweet:\n",
        "  h = df['TweetMinusStopWords'].apply(lambda x: x.count(i))  \n",
        "  d = pd.DataFrame(data={i: h})\n",
        "  d['TimeBin'] = df.TimeBin\n",
        "  g = d.groupby(\"TimeBin\")\n",
        "  g = g[i].sum()\n",
        "  hourly = pd.concat([hourly, g], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YIr4wGIKHsg"
      },
      "source": [
        "hourly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtPclVuhKhsC"
      },
      "source": [
        "hourly.drop('Hour', axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11WrkB4DKq0d"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "hourly.iplot(yTitle='Count', xTitle = 'Hour of Day', title='Hourly Variation of Top Trigrams')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Os8HTdGjpe_"
      },
      "source": [
        "daily = pd.DataFrame(data={'Day': np.linspace(0, 7, 8)})\n",
        "for i in df4.Tweet:\n",
        "  h = df['TweetMinusStopWords'].apply(lambda x: x.count(i))  \n",
        "  d = pd.DataFrame(data={i: h})\n",
        "  d['TimeBin'] = pd.DatetimeIndex(df['Date']).weekday\n",
        "  g = d.groupby(\"TimeBin\")\n",
        "  g = g[i].sum()\n",
        "  daily = pd.concat([daily, g], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7McUgM3Kj4Oa"
      },
      "source": [
        "daily"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcVH-7HEj4wa"
      },
      "source": [
        "daily.drop('Day', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0unlonakYff"
      },
      "source": [
        "configure_plotly_browser_state()\n",
        "daily.iplot(yTitle='Count', xTitle = 'Day of Week', title='Daily Variation of Top Trigrams')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z7Udev3FEz6"
      },
      "source": [
        "# Arindam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzSnV3Z17b3s"
      },
      "source": [
        "##Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK0hAsi3Lh4j"
      },
      "source": [
        "#@title Default title text\n",
        "!pip install chart_studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOS2DiNTLxLp"
      },
      "source": [
        "!pip install scattertext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k1g9q_1MXIR"
      },
      "source": [
        "!python -m spacy download en_core_web_md "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmT233fCC38Q"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3-8-5CFKTQt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
        "import plotly.graph_objs as go\n",
        "import chart_studio\n",
        "import cufflinks\n",
        "pd.options.display.max_columns = 30\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import plotly.figure_factory as ff\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "from plotly.offline import iplot\n",
        "cufflinks.go_offline()\n",
        "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "from collections import Counter\n",
        "import scattertext as st\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import en_core_web_sm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1zEe2ucnZv7"
      },
      "source": [
        "from wordcloud import WordCloud\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_Zl0DajFbI"
      },
      "source": [
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqFClQboSspm"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# type d => <enter> => stopwords => <enter>\n",
        "# type d => <enter> => wordnet => <enter> => q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNPNK5hISxmY"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean(txt: str):\n",
        "  txt = re.sub(r'@[A-Za-z0-9_]+', '', txt)\n",
        "  txt = re.sub(r'&amp', 'and', txt)\n",
        "  txt = re.sub('#', '', txt)\n",
        "  txt = re.sub(r'RT', '', txt)\n",
        "  txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', txt)\n",
        "  txt = ''.join([c for c in txt if c not in string.punctuation])\n",
        "  txt = txt.lower()\n",
        "  #for c in string.punctuation:\n",
        "  #  txt = re.sub(c, '', txt)\n",
        "  return txt\n",
        "\n",
        "def tokenize(text: str):\n",
        "  tokens = re.split('\\W+', text)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdb3RRi5S1Tu"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def removeStopwords(tokens):\n",
        "  ret = [token for token in tokens if token not in stopwords]\n",
        "  ret = [token for token in tokens if token != '']\n",
        "  return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIwWg688S6W8"
      },
      "source": [
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "  ret = [wn.lemmatize(token) for token in tokens]\n",
        "  return ret\n",
        "\n",
        "def finalClean(txt: str):\n",
        "  return lemmatize(removeStopwords(tokenize(clean(txt))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj3WF5MRLqdx"
      },
      "source": [
        "df18 = pd.read_csv('data1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9wdAQynp7UI"
      },
      "source": [
        "df2 = pd.read_csv('data2.csv')\n",
        "df3 = pd.read_csv('data3.csv')\n",
        "df4 = pd.read_csv('data4.csv')\n",
        "df5 = pd.read_csv('data5.csv')\n",
        "df6 = pd.read_csv('data6.csv')\n",
        "df7 = pd.read_csv('data7.csv')\n",
        "df8 = pd.read_csv('data8.csv')\n",
        "df9 = pd.read_csv('data9.csv')\n",
        "df10 = pd.read_csv('data10.csv')\n",
        "df11 = pd.read_csv('data11.csv')\n",
        "df12 = pd.read_csv('data12.csv')\n",
        "df13 = pd.read_csv('data13.csv')\n",
        "df14 = pd.read_csv('data14.csv')\n",
        "df15 = pd.read_csv('data15.csv')\n",
        "df16 = pd.read_csv('data16.csv')\n",
        "df17 = pd.read_csv('data17.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHbK92GLqM5q"
      },
      "source": [
        "df = pd.concat([df18,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7UdciAXRp8a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w66tC0geS6Zi"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsU3oQnXS9pZ"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5g0v5HBRsLR"
      },
      "source": [
        "df.columns = ['target','id','time','query','username','tweet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5OgjTjLR7XZ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSJ2AMQhxNzg"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF6-2s6HR8U5"
      },
      "source": [
        "df['target'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJKZI8BuTH8z"
      },
      "source": [
        "sns.countplot(x ='target', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1prpFTISHRq"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21f94HVhgD4z"
      },
      "source": [
        "df['tweet_without_stopwords'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT6P7Od_gra7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG6w7NDGhc0i"
      },
      "source": [
        "df['final'] = df['tweet_without_stopwords'].apply(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIDxhTJwhfu6"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P5S4fr8hguB"
      },
      "source": [
        "\n",
        "df['review_len'] = df['tweet'].astype(str).apply(len)\n",
        "df['word_count'] = df['tweet'].apply(lambda x: len(str(x).split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hfLGqfKhsxp"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46o0qfHrhjLY"
      },
      "source": [
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)1+', r'1', text)\n",
        "df['final1'] = df['final'].apply(lambda x: cleaning_repeating_char(x))\n",
        "df['final1'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWzZptJIiBPb"
      },
      "source": [
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
        "df['final'] = df['final1'].apply(lambda x: cleaning_URLs(x))\n",
        "df['final'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Rd10wmxW2r"
      },
      "source": [
        "df.drop(['final1','final2','tweet_without_stopwords'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oetLLQ3EjTwW"
      },
      "source": [
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNWUATzejKmG"
      },
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_words(df['final'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n",
        "df1 = pd.DataFrame(common_words, columns = ['tweet' , 'count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9exssmqbkBk9"
      },
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_words(df['tweet'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n",
        "df2 = pd.DataFrame(common_words, columns = ['tweet' , 'count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs7UKAB_kNif"
      },
      "source": [
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_bigram(df['final'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqkEjtHqkuKO"
      },
      "source": [
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_bigram(df['tweet'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n",
        "df4 = pd.DataFrame(common_words, columns = ['tweet' , 'count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEoG3843k-Xl"
      },
      "source": [
        "dataplot = sns.heatmap(df.corr(), cmap = \"YlGnBu\", annot = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIOoIyxzmG0F"
      },
      "source": [
        "def get_top_n_trigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_trigram(df['final'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tumJrzcmU7B"
      },
      "source": [
        "common_words = get_top_n_trigram(df['tweet'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap7trFqjY5b1"
      },
      "source": [
        "from datetime import datetime \n",
        "df[\"datetime\"] = pd.to_datetime(df[\"time\"],format=\"%a %b %d %H:%M:%S PDT %Y\")\n",
        "df[\"date\"] = df[\"datetime\"].dt.date\n",
        "df[\"time\"] = df[\"datetime\"].dt.time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KDNJ1zWY6Iw"
      },
      "source": [
        "df = df.drop(['tweet_without_stopwords','final1','final2'], axis = 1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sg3kru83Qr7"
      },
      "source": [
        "##Time Series Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqxd8guIY6mX"
      },
      "source": [
        "sns.countplot(x = df[\"datetime\"].dt.month,data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H698awsY7Bb"
      },
      "source": [
        "sns.countplot(x = df[\"datetime\"].dt.month, hue = 'target', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqMOsDCY7eU"
      },
      "source": [
        "sns.set(rc = {'figure.figsize':(30,10)})\n",
        "sns.countplot(x = df[\"date\"], hue = 'target', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jE-GsPP5Lb5"
      },
      "source": [
        "df1 = df[df[\"datetime\"].dt.month == 4]\n",
        "sns.countplot(x = df1[\"datetime\"].dt.day, hue = 'target', data = df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKChUkJQaSKb"
      },
      "source": [
        "df1 = df[df[\"datetime\"].dt.month == 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNb6iQ4GaSi6"
      },
      "source": [
        "sns.countplot(x = df1[\"datetime\"].dt.day, hue = 'target', data = df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8-wZu2a6Cs4"
      },
      "source": [
        "df1 = df[df[\"datetime\"].dt.month == 6]\n",
        "sns.countplot(x = df1[\"datetime\"].dt.day, hue = 'target', data = df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLr89ePwY76o"
      },
      "source": [
        "sns.countplot(x = df[\"datetime\"].dt.day_name(), hue = 'target', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzN4U_5Vnm6j"
      },
      "source": [
        "sns.countplot(x = df[\"datetime\"].dt.hour, hue = 'target', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mNYMR3EFHNo"
      },
      "source": [
        "#Lokendra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpCFqdk8iwG6",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "%cd ./drive/My\\ Drive/\n",
        "%cd ./ime672/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3b7JS6CdWEW"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('sample_data2.csv')                         # Spell correct data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSaiJ_rTd5Ln"
      },
      "source": [
        "## Preprocessing and Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E-KbgywkaFX"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "#removing @username\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "#removind urls starting with https\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "#removind urls starting with www\n",
        "pat3 = r'www.[^ ]+'\n",
        "combined_pat = r'|'.join((pat1, pat2, pat3))\n",
        "# stop words \n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "# puctuation\n",
        "punct = set(string.punctuation)\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-rl9s4gkmMw"
      },
      "source": [
        "# clean text data\n",
        "# remove non alphabetic characters\n",
        "# remove stopwords and lemmatize\n",
        " \n",
        "def clean_text(sentence):\n",
        "    soup = BeautifulSoup(sentence, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped).strip()\n",
        "    try:\n",
        "        sentence = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        sentence = stripped\n",
        "    \n",
        "    sentence = sentence.lower()\n",
        "    sentence = neg_pattern.sub(lambda x: negations_dic[x.group()], sentence)\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "    \n",
        "    # Tokenize\n",
        "    word_list = word_tokenize(sentence)\n",
        "    \n",
        " \n",
        "    # remove stop words\n",
        "    word_list = [word for word in word_list if word not in stopwords_list]\n",
        "    # remove very small words, length < 3 as they don't contribute any useful information\n",
        "    word_list = [word for word in word_list if len(word) > 2]\n",
        "    # remove punctuation\n",
        "    word_list = [word for word in word_list if word not in punct]\n",
        "    \n",
        "    # stemming\n",
        "    # ps  = PorterStemmer()\n",
        "    # word_list = [ps.stem(word) for word in word_list]\n",
        "    \n",
        "    # lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    word_list = [lemma.lemmatize(word) for word in word_list]\n",
        "    # list to sentence\n",
        "    # sentence = ' '.join(word_list).strip()\n",
        "    \n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL2rcNHoksUB"
      },
      "source": [
        "tweet_list = df['spell_corrected_sentence'].tolist()\n",
        "word_list = clean_text(''.join(tweet_list))\n",
        "word_list[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIQfgPateTB-"
      },
      "source": [
        "## Data Analiysis of Spell correct data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dtFXK_Uewpo"
      },
      "source": [
        "### **Unigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QAdJCsLlOAI"
      },
      "source": [
        "unigrams_top15 = (pd.Series(nltk.ngrams(word_list, 1)).value_counts())[:15]\n",
        "print(unigrams_top15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIUUC_rFpwVR"
      },
      "source": [
        "unigrams_top15.sort_values().plot.barh(color='blue', width=.9, figsize=(15, 10))\n",
        "plt.title('15 Most Frequently Occuring Unigrams')\n",
        "plt.ylabel('Unigram')\n",
        "plt.xlabel('# of Occurances')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8UYhffdfDM1"
      },
      "source": [
        "### **Bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrqH-jdsk0A_"
      },
      "source": [
        "bigrams_top15 = (pd.Series(nltk.ngrams(word_list, 2)).value_counts())[:15]\n",
        "print(bigrams_top15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB-ayjdEk0K9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bigrams_top15.sort_values().plot.barh(color='blue', width=.9, figsize=(15, 10))\n",
        "plt.title('15 Most Frequently Occuring Bigrams')\n",
        "plt.ylabel('Bigram')\n",
        "plt.xlabel('# of Occurances')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzezarhPfMfx"
      },
      "source": [
        "### **Trigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELHvyKsbpznJ"
      },
      "source": [
        "trigrams_top15 = (pd.Series(nltk.ngrams(word_list, 3)).value_counts())[:20]\n",
        "print(trigrams_top15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vobimXcYp4FR"
      },
      "source": [
        "trigrams_top15.sort_values().plot.barh(color='blue', width=.9, figsize=(15, 10))\n",
        "plt.title('20 Most Frequently Occuring Trigrams')\n",
        "plt.ylabel('Trigram')\n",
        "plt.xlabel('# of Occurances')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoZ1HWuhp7i1"
      },
      "source": [
        "df['clean_tweet'] = [clean_text(t) for t in df['spell_corrected_sentence']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMm6oYNw1DWb"
      },
      "source": [
        "class0 = df[df[\"polarity\"] == 0]\n",
        "class4 = df[df[\"polarity\"] == 4]\n",
        "class0.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93MYZfgggju"
      },
      "source": [
        "## **WordCloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc4Pw6Zb1RzJ"
      },
      "source": [
        "from nltk import flatten\n",
        "text = ' '.join(flatten(df[\"clean_tweet\"].to_list()))\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(text)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEQ86D_fhuj0"
      },
      "source": [
        "### **WordCloud for Class 0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YClAuDtD7GNx"
      },
      "source": [
        "text = ' '.join(flatten(class0[\"clean_tweet\"].to_list()))\n",
        "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(text)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0jflxfXh4f9"
      },
      "source": [
        "### **WordCloud for Class 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ivCYPWGn55"
      },
      "source": [
        "text = ' '.join(flatten(class4[\"clean_tweet\"].to_list()))\n",
        "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(text)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl9Du1NRjDja"
      },
      "source": [
        "## Analysis of Important Dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf8qXAl0kByy"
      },
      "source": [
        "import pandas as pd\n",
        "colnames=['class', 'id', 'time', 'query','tag','text'] \n",
        "data = pd.read_csv('training.csv',encoding='ISO-8859-1',names=colnames)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekLcDDx1_ENt"
      },
      "source": [
        "data['clean_tweet'] = [clean_text(t) for t in data['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuaYRem1GrXS"
      },
      "source": [
        "import datetime \n",
        "df=data\n",
        "df[\"datetime\"] = pd.to_datetime(df[\"time\"],format=\"%a %b %d %H:%M:%S PDT %Y\")\n",
        "df[\"date\"] = df[\"datetime\"].dt.date\n",
        "df[\"time\"] = df[\"datetime\"].dt.time\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mhErPdE54zT"
      },
      "source": [
        "### **Mother's Day**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfc8j-c9JZFv"
      },
      "source": [
        "df1 = df[df['date']==datetime.date(2009, 5, 10)]\n",
        "# Plotting the distribution for dataset.\n",
        "import seaborn as sns\n",
        "sns.countplot(x='class', data=df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoW-UDbr9osF"
      },
      "source": [
        "**Topic Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXgRUAP-NoM7"
      },
      "source": [
        "n_topics=10\n",
        "def get_keys(topic_matrix):\n",
        "    '''\n",
        "    returns an integer list of predicted topic \n",
        "    categories for a given topic matrix\n",
        "    '''\n",
        "    keys = topic_matrix.argmax(axis=1).tolist()\n",
        "    return keys\n",
        "def keys_to_counts(keys):\n",
        "    '''\n",
        "    returns a tuple of topic categories and their \n",
        "    accompanying magnitudes for a given list of keys\n",
        "    '''\n",
        "    count_pairs = Counter(keys).items()\n",
        "    categories = [pair[0] for pair in count_pairs]\n",
        "    counts = [pair[1] for pair in count_pairs]\n",
        "    return (categories, counts)\n",
        "def get_top_n_words(n, keys, document_term_matrix, tfidf_vectorizer):\n",
        "    '''\n",
        "    returns a list of n_topic strings, where each string contains the n most common \n",
        "    words in a predicted category, in order\n",
        "    '''\n",
        "    top_word_indices = []\n",
        "    for topic in range(n_topics):\n",
        "        temp_vector_sum = 0\n",
        "        for i in range(len(keys)):\n",
        "            if keys[i] == topic:\n",
        "                temp_vector_sum += document_term_matrix[i]\n",
        "        temp_vector_sum = temp_vector_sum.toarray()\n",
        "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
        "        top_word_indices.append(top_n_word_indices)   \n",
        "    top_words = []\n",
        "    for topic in top_word_indices:\n",
        "        topic_words = []\n",
        "        for index in topic:\n",
        "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
        "            temp_word_vector[:,index] = 1\n",
        "            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
        "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
        "        top_words.append(\" \".join(topic_words))         \n",
        "    return top_words\n",
        "    \n",
        "    top_n_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR0sLu3gLAQq"
      },
      "source": [
        "n_topics=10\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "reindexed_data6 = df1['text']\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
        "reindexed_data6 = reindexed_data6.values\n",
        "document_term_matrix6 = tfidf_vectorizer.fit_transform(reindexed_data6)\n",
        "\n",
        "lsa_model = TruncatedSVD(n_components=10)\n",
        "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix6)\n",
        "\n",
        "lsa_keys = get_keys(lsa_topic_matrix)\n",
        "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
        "\n",
        "top_n_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix6, tfidf_vectorizer)\n",
        "\n",
        "for i in range(len(top_n_words_lsa)):\n",
        "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])\n",
        "\n",
        "top_3_words = get_top_n_words(3, lsa_keys, document_term_matrix6, tfidf_vectorizer)\n",
        "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.bar(lsa_categories, lsa_counts);\n",
        "ax.set_xticks(lsa_categories);\n",
        "ax.set_xticklabels(labels);\n",
        "ax.set_ylabel('Number of tweets');\n",
        "ax.set_title('LSA topic counts');\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9q984aGiCQc"
      },
      "source": [
        "### **Father's day**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xz4ZMpEPKHU"
      },
      "source": [
        "df1 = df[df['date']==datetime.date(2009, 6, 21)]\n",
        "# Plotting the distribution for dataset.\n",
        "import seaborn as sns\n",
        "sns.countplot(x='class', data=df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bQLqBHXiO3n"
      },
      "source": [
        "**Topic Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Ijm_JmGd-D"
      },
      "source": [
        "n_topics=10\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "reindexed_data6 = df1['text']\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
        "reindexed_data6 = reindexed_data6.values\n",
        "document_term_matrix6 = tfidf_vectorizer.fit_transform(reindexed_data6)\n",
        "\n",
        "lsa_model = TruncatedSVD(n_components=10)\n",
        "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix6)\n",
        "\n",
        "lsa_keys = get_keys(lsa_topic_matrix)\n",
        "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
        "\n",
        "top_n_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix6, tfidf_vectorizer)\n",
        "\n",
        "for i in range(len(top_n_words_lsa)):\n",
        "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])\n",
        "\n",
        "top_3_words = get_top_n_words(3, lsa_keys, document_term_matrix6, tfidf_vectorizer)\n",
        "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.bar(lsa_categories, lsa_counts);\n",
        "ax.set_xticks(lsa_categories);\n",
        "ax.set_xticklabels(labels);\n",
        "ax.set_ylabel('Number of tweets');\n",
        "ax.set_title('LSA topic counts');\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Evi8g0OGnUf"
      },
      "source": [
        "for t in df1[\"text\"]:                                    # all tweets include about father's day\n",
        "  if 'fathers day' in t:\n",
        "    print(t)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}